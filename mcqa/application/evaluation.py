import re

from mcqa.domain.evaluation import Evaluation


class QAEvaluation(Evaluation):
    """Class for evaluating generated answers against actual answers."""

    def _clean_text(self, text: str) -> str:
        """Cleans the text by removing non-ASCII characters and extra whitespace, and converting to lowercase.

        Args:
            text (str): The text to be cleaned.

        Returns:
            str: The cleaned text.
        """
        return re.sub(r"\s+", " ", re.sub(r"[^\x20-\x7E]", "", text)).strip().lower()

    def evaluate(self, generated_answer: str, actual_answer: str) -> float:
        """Evaluates the generated answer against the actual answer.

        Args:
            generated_answer (str): The answer generated by the model.
            actual_answer (str): The correct answer to compare against.

        Returns:
            float: The evaluation score, 100.0 if the answers match, otherwise 0.0.
        """
        if self._clean_text(generated_answer) == self._clean_text(actual_answer):
            return 100.0
        return 0.0
